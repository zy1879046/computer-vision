{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61b57eaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: download in /Users/zhangyu/anaconda3/lib/python3.11/site-packages (0.3.5)\r\n",
      "Requirement already satisfied: tqdm in /Users/zhangyu/anaconda3/lib/python3.11/site-packages (from download) (4.65.0)\r\n",
      "Requirement already satisfied: six in /Users/zhangyu/anaconda3/lib/python3.11/site-packages (from download) (1.16.0)\r\n",
      "Requirement already satisfied: requests in /Users/zhangyu/anaconda3/lib/python3.11/site-packages (from download) (2.31.0)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/zhangyu/anaconda3/lib/python3.11/site-packages (from requests->download) (2.0.4)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/zhangyu/anaconda3/lib/python3.11/site-packages (from requests->download) (3.4)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/zhangyu/anaconda3/lib/python3.11/site-packages (from requests->download) (1.26.16)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/zhangyu/anaconda3/lib/python3.11/site-packages (from requests->download) (2023.7.22)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "091f4d31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating data folder...\n",
      "Downloading data from https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/notebook/datasets/MNIST_Data.zip (10.3 MB)\n",
      "\n",
      "file_sizes: 100%|██████████████████████████| 10.8M/10.8M [00:00<00:00, 14.5MB/s]\n",
      "Extracting zip file...\n",
      "Successfully downloaded / unzipped to ./data/\n"
     ]
    }
   ],
   "source": [
    "from download import download\n",
    "\n",
    "url = \"https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/\" \\\n",
    "      \"notebook/datasets/MNIST_Data.zip\"\n",
    "path = download(url, \"./data/\", kind=\"zip\", replace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6dc1a88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import mindspore\n",
    "from mindspore.common.initializer import Normal\n",
    "from mindspore.dataset import MnistDataset, vision\n",
    "from mindspore import nn, LossMonitor, TimeMonitor\n",
    "from mindspore.dataset.vision import Inter\n",
    "from mindspore.train import Model, CheckpointConfig, ModelCheckpoint, Callback\n",
    "import mindspore.dataset.transforms as transforms\n",
    "from mindspore import dtype as mstype\n",
    "mindspore.set_context(mode=mindspore.GRAPH_MODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "13ccd23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(data_path, Train = True,batch_size=32, repeat_size=1):\n",
    "    \"\"\"\n",
    "    创建用于训练的MNIST数据集。\n",
    "\n",
    "    此函数负责加载MNIST数据集，对数据进行预处理和转换，以便它们可以用于训练神经网络。数据预处理包括调整图像大小、重新缩放和类型转换。\n",
    "\n",
    "    参数:\n",
    "        data_path (str): MNIST数据集的路径。这应该是包含MNIST数据文件的目录路径。\n",
    "        Train (bool):加载什么数据集，若为True 加载训练数据集，否则加载测试数据集\n",
    "        batch_size (int, 可选): 每个数据批次的大小。默认值为32。\n",
    "        repeat_size (int, 可选): 数据集重复的次数。这用于增加数据集的大小。默认值为1。\n",
    "\n",
    "    步骤:\n",
    "        1. 加载MNIST数据集。\n",
    "        2. 对图像执行大小调整操作，将图像大小统一调整为32x32像素。\n",
    "        3. 对图像进行重新缩放和标准化处理。先将像素值缩放到0-1之间，然后进行标准化。\n",
    "        4. 将图像的格式从高宽通道(HWC)转换为通道高宽(CHW)。\n",
    "        5. 对标签进行类型转换，将其转换为整型（int32）。\n",
    "        6. 对数据集进行洗牌、批处理和重复操作，以准备训练过程。\n",
    "\n",
    "    返回:\n",
    "        返回一个处理过的MNIST数据集，可以直接用于模型训练。\n",
    "\n",
    "    注意:\n",
    "        - 数据集的预处理步骤对于训练深度学习模型来说是非常重要的，它们会影响训练的效果和速度。\n",
    "        - 调整batch_size和repeat_size可以影响模型训练时的内存消耗和速度。\n",
    "    \"\"\"\n",
    "    data_train = os.path.join(data_path, 'train')\n",
    "    data_test = os.path.join(data_path, 'test')\n",
    "    mnist_dataset = MnistDataset(data_train if Train==True else data_test )\n",
    "\n",
    "    resize_operation = vision.Resize((32, 32), interpolation=Inter.LINEAR)\n",
    "    rescale_normalization_op = vision.Rescale(1 / 0.3081, -1 * 0.1307 / 0.3081)\n",
    "    rescale_op = vision.Rescale(1.0 / 255.0, 0.0)\n",
    "    hwc_to_chw_op = vision.HWC2CHW()\n",
    "    type_cast_op = transforms.TypeCast(mstype.int32)\n",
    "\n",
    "    mnist_dataset = mnist_dataset.map(input_columns=\"label\", operations=type_cast_op)\n",
    "    mnist_dataset = mnist_dataset.map(input_columns=\"image\",\n",
    "                                      operations=[resize_operation, rescale_op, rescale_normalization_op,\n",
    "                                                  hwc_to_chw_op])\n",
    "    mnist_dataset = mnist_dataset.shuffle(buffer_size=10000)\n",
    "    mnist_dataset = mnist_dataset.batch(batch_size, drop_remainder=True)\n",
    "    mnist_dataset = mnist_dataset.repeat(repeat_size)\n",
    "\n",
    "    return mnist_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93dbc8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet5(nn.Cell):\n",
    "    \"\"\"\n",
    "    LeNet-5 神经网络结构。\n",
    "\n",
    "    这是一个经典的卷积神经网络，通常用于图像识别任务。它包含了两个卷积层和三个全连接层。\n",
    "\n",
    "    参数:\n",
    "        num_class (int): 输出层的类别数量。默认为10，适用于MNIST数据集。\n",
    "        num_channel (int): 输入图像的通道数。对于灰度图像，此值为1。\n",
    "\n",
    "    组件:\n",
    "        - conv1: 第一个卷积层，使用有效填充。\n",
    "        - conv2: 第二个卷积层，同样使用有效填充。\n",
    "        - fc1: 第一个全连接层。\n",
    "        - fc2: 第二个全连接层。\n",
    "        - fc3: 第三个全连接层，输出层。\n",
    "        - relu: 激活函数，使用ReLU。\n",
    "        - max_pool2d: 最大池化层。\n",
    "        - flatten: 扁平化层，用于全连接层之前的数据转换。\n",
    "\n",
    "    方法:\n",
    "        - construct(x): 定义了前向传播的过程。\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_class=10, num_channel=1):\n",
    "        super(LeNet5, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(num_channel, 6, 5, pad_mode='valid')\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5, pad_mode='valid')\n",
    "        self.fc1 = nn.Dense(16 * 5 * 5, 120, weight_init=Normal(0.02))\n",
    "        self.fc2 = nn.Dense(120, 84, weight_init=Normal(0.02))\n",
    "        self.fc3 = nn.Dense(84, num_class, weight_init=Normal(0.02))\n",
    "        self.relu = nn.ReLU()\n",
    "        self.max_pool2d = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "    def construct(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.max_pool2d(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.max_pool2d(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5e415071",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(model, epoch_size, data_path, repeat_size, checkpoint_callback):\n",
    "    \"\"\"\n",
    "    训练神经网络模型。\n",
    "\n",
    "    此函数负责初始化数据集，然后使用指定的模型进行训练。在训练过程中，它将记录损失并保存模型的检查点。\n",
    "\n",
    "    参数:\n",
    "        model (Model): 要训练的神经网络模型。\n",
    "        epoch_size (int): 训练过程中遍历数据集的次数。\n",
    "        data_path (str): 训练数据集的路径。\n",
    "        repeat_size (int): 数据集的重复次数，用于扩充数据集。\n",
    "        checkpoint_callback (Callback): 用于保存模型检查点的回调函数。\n",
    "\n",
    "    过程:\n",
    "        - 使用 `create_dataset` 函数创建训练数据集。\n",
    "        - 调用模型的 `train` 方法进行训练。\n",
    "        - 在训练过程中，会通过回调函数记录损失和保存检查点。\n",
    "\n",
    "    注意:\n",
    "        - 确保提供的 `data_path` 包含适当格式的数据。\n",
    "    \"\"\"\n",
    "    print(\"============== 开始训练 ==============\")\n",
    "    ds_train = create_dataset(data_path, True,32, repeat_size)\n",
    "    ds_eval = create_dataset(data_path, False, 32, repeat_size)\n",
    "    model.train(epoch_size, ds_train, callbacks=[checkpoint_callback, LossMonitor(per_print_times=ds_train.get_dataset_size())],\n",
    "                dataset_sink_mode=False)\n",
    "    metrics_result = model.eval(ds_eval)\n",
    "    print('Accuracy:', metrics_result[\"Accuracy\"])\n",
    "    print(\"============== 训练结束 ==============\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6ddb5e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== 开始训练 ==============\n",
      "epoch: 1 step: 1875, loss is 0.017459269613027573\n",
      "epoch: 2 step: 1875, loss is 0.09903346002101898\n",
      "epoch: 3 step: 1875, loss is 0.00017907457367982715\n",
      "epoch: 4 step: 1875, loss is 0.008635335601866245\n",
      "epoch: 5 step: 1875, loss is 0.19991040229797363\n",
      "epoch: 6 step: 1875, loss is 0.004170055966824293\n",
      "epoch: 7 step: 1875, loss is 0.09278905391693115\n",
      "epoch: 8 step: 1875, loss is 0.0019566493574529886\n",
      "epoch: 9 step: 1875, loss is 0.0005295849987305701\n",
      "epoch: 10 step: 1875, loss is 0.0018454601522535086\n",
      "Accuracy: 0.9878806089743589\n",
      "============== 训练结束 ==============\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "data_url = \"./data/MNIST_DATA\"\n",
    "output_path = \"./check\"\n",
    "net = LeNet5()\n",
    "net_loss = nn.SoftmaxCrossEntropyWithLogits(sparse=True, reduction=\"mean\")\n",
    "net_opt = nn.Momentum(net.trainable_params(), 0.01, 0.9)\n",
    "\n",
    "model = Model(net, net_loss, net_opt, metrics={\"Accuracy\": nn.Accuracy()})\n",
    "\n",
    "config_checkpoint = CheckpointConfig(save_checkpoint_steps=1875, keep_checkpoint_max=10)\n",
    "checkpoint_callback = ModelCheckpoint(prefix=\"checkpoint_lenet\", directory=output_path,\n",
    "                                      config=config_checkpoint)\n",
    "\n",
    "train_network(model, epochs, data_url, 1, checkpoint_callback)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5192677",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mindspore",
   "language": "python",
   "name": "mindspore"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
